#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Oct 19 11:09:18 2020

@author: yangqianwan
"""

import sys
sys.path.append(r'/Users/yangqianwan/Desktop/lab/NET/data')
from unet_2d import unet_2d
import torch
import torch.optim as optim
from torch.optim import lr_scheduler
import time
import copy
from dataset import MyDataset
from torch.utils.data import DataLoader

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
trans = transforms.Compose([
    transforms.ToTensor(),
])

whole_set=MyDataset(transform = trans)
length=len(whole_set)
train_size,validate_size=2450,len(whole_set)-2450
train_set,validate_set=data.random_split(whole_set,[train_size,validate_size])
image_datasets = {
    'train': train_set, 'val': validate_set
}

batch_size = 1
dataloaders = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)
model = unet_2d()
#batch_size = 4
#dataloaders = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),
# Observe that all parameters are being optimized
optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()
for epoch in range(1):
    running_loss = 0.0
    for inputs, labels in dataloaders:
        inputs = inputs
        labels = labels
        #print(inputs)
        # zero the parameter gradients
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if i % 2000 == 1999:
            print('[%d, %5d] loss: %.3f' %
            (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0
print('Finished Training')
